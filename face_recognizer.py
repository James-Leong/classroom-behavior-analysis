import io
import logging
import pickle

from contextlib import redirect_stdout, redirect_stderr, contextmanager
from pathlib import Path
from typing import List, Dict, Tuple, Optional

import cv2
import numpy as np
import torch
import warnings
import os

from PIL import Image, ImageDraw, ImageFont
from insightface.app import FaceAnalysis
from ultralytics import YOLO

# å¿½ç•¥ albumentations æ£€æŸ¥ç‰ˆæœ¬æ—¶ç½‘ç»œè¶…æ—¶çš„æ— å®³è­¦å‘Š
warnings.filterwarnings(
    "ignore",
    message="Error fetching version info",
    category=UserWarning,
    module=r"albumentations.*",
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# å¸¸è§ç³»ç»Ÿå­—ä½“å€™é€‰ï¼ˆmacOS/Windows/Linuxï¼‰ï¼ŒæŒ‰éœ€æ‰©å±•
FONT_LIST = [
    # macOS
    "/System/Library/Fonts/STHeiti Medium.ttc",
    "/System/Library/Fonts/STHeiti Light.ttc",
    "/System/Library/Fonts/AppleGothic.ttf",
    "/Library/Fonts/Arial Unicode.ttf",
    "/System/Library/Fonts/Supplemental/STHeiti.ttf",
    # Windows (æ³¨æ„å­—ç¬¦ä¸²ä¸­çš„åæ–œæ å·²è½¬ä¹‰)
    "C:\\Windows\\Fonts\\msyh.ttc",
    "C:\\Windows\\Fonts\\msyh.ttf",
    "C:\\Windows\\Fonts\\simsun.ttc",
    "C:\\Windows\\Fonts\\arialuni.ttf",
    # å¸¸è§ Linux å­—ä½“
    "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
    "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
    "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",
    "/usr/share/fonts/truetype/freefont/FreeSans.ttf",
    "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
]


def draw_text_cn(img: np.ndarray, text: str, org: Tuple[int, int], font_size: int = 14, color=(255, 255, 255)):
    """
    åœ¨ OpenCV å›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡æˆ–å…¶ä»– Unicode æ–‡æœ¬ï¼ˆä¼˜å…ˆä½¿ç”¨ PILï¼‰ï¼Œè‹¥å¤±è´¥åˆ™å›é€€åˆ° cv2.putTextã€‚

    Args:
        img: BGR æ ¼å¼çš„ numpy å›¾åƒ
        text: è¦ç»˜åˆ¶çš„æ–‡æœ¬ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        org: æ–‡æœ¬å·¦ä¸Šè§’ä½ç½® (x, y)
        font_size: å­—ä½“å¤§å°ï¼ˆåƒç´ ï¼‰
        color: BGR é¢œè‰²å…ƒç»„
    """
    try:
        # è½¬ä¸º PIL (RGB)
        pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_img)

        font = None
        for p in FONT_LIST:
            try:
                font = ImageFont.truetype(p, font_size)
                break
            except Exception:
                continue

        if font is None:
            font = ImageFont.load_default()

        # PIL ä½¿ç”¨ RGBï¼Œcolor ä¸º BGRï¼Œéœ€è½¬æ¢
        rgb_color = (color[2], color[1], color[0])
        draw.text(org, text, font=font, fill=rgb_color)

        # å†™å›åˆ° numpy BGR å›¾åƒï¼ˆåŸåœ°æ›¿æ¢å†…å®¹ï¼‰
        img[:] = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

    except Exception:
        # å›é€€ï¼šä½¿ç”¨ OpenCV çš„ putTextï¼ˆå¯èƒ½ä¸­æ–‡ä»ç„¶ä¹±ç ï¼‰
        # è¿™é‡Œå°½åŠ›ä¿æŒä¸ PIL ç›¸è¿‘çš„å¤§å°æ˜ å°„
        font_scale = max(0.3, font_size / 24.0)
        cv2.putText(img, text, org, cv2.FONT_HERSHEY_SIMPLEX, font_scale, (color[0], color[1], color[2]), 1, cv2.LINE_AA)


def measure_text_cn(text: str, font_size: int = 14) -> Tuple[int, int]:
    """ä½¿ç”¨ PIL æµ‹é‡æ–‡æœ¬åƒç´ å°ºå¯¸ï¼Œå›é€€åˆ° OpenCV æµ‹é‡ã€‚"""
    try:

        font = None
        for p in FONT_LIST:
            try:
                font = ImageFont.truetype(p, font_size)
                break
            except Exception:
                continue

        if font is None:
            font = ImageFont.load_default()

        # ä½¿ç”¨ textbbox å¾—åˆ°ç²¾ç¡®å°ºå¯¸
        dummy = Image.new('RGB', (10, 10))
        draw = ImageDraw.Draw(dummy)
        bbox = draw.textbbox((0, 0), text, font=font)
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        return int(width), int(height)

    except Exception:
        # å›é€€ï¼šOpenCV è¿‘ä¼¼æµ‹é‡
        font_scale = max(0.3, font_size / 24.0)
        (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1)
        return int(w), int(h)


@contextmanager
def _suppress_fds():
    """Context manager that redirects FD 1 and 2 to /dev/null.

    This suppresses output from C-level prints and other threads that bypass
    Python's sys.stdout/sys.stderr objects.
    """
    devnull = os.open(os.devnull, os.O_RDWR)
    old_stdout = os.dup(1)
    old_stderr = os.dup(2)
    try:
        os.dup2(devnull, 1)
        os.dup2(devnull, 2)
        yield
    finally:
        os.dup2(old_stdout, 1)
        os.dup2(old_stderr, 2)
        os.close(devnull)
        os.close(old_stdout)
        os.close(old_stderr)


class FaceRecognizer:
    """
    äººè„¸è¯†åˆ«å™¨ï¼ŒåŸºäºYolov11å’ŒInsightFaceæ¨¡å‹å®ç°

    ä¸»è¦åŠŸèƒ½ï¼š
    1. åŸºäºyoloçš„äººè„¸æ£€æµ‹
    2. äººè„¸å›¾åƒçš„è‡ªåŠ¨åˆ‡å‰²ä¸è°ƒæ•´
    3. åŸºäºinsightfaceçš„äººè„¸è¯†åˆ«
    """

    def __init__(
        self,
        detection_model: str = 'yolo11n.pt',
        recognition_model: str = 'buffalo_l',
        gallery_path: str = 'gallery',
        threshold: float = 0.4,
        quality_threshold: float = 0.7,
        det_size: int = 320,
        device: str = 'auto',
        rebuild_gallery: bool = False,
        yolo_conf: float = 0.12,
    ):
        """
        åˆå§‹åŒ–äººè„¸è¯†åˆ«å™¨

        Args:
            detection_model: YOLOæ£€æµ‹æ¨¡å‹è·¯å¾„æˆ–åç§°ï¼Œé»˜è®¤'yolo11n.pt'
            recognition_model: InsightFaceæ¨¡å‹åç§°ï¼Œé»˜è®¤'buffalo_l'
            gallery_path: å›¾åº“è·¯å¾„ï¼ŒåŒ…å«æŒ‰äººå‘˜å‘½åçš„å­ç›®å½•ï¼Œæ¯ä¸ªå­ç›®å½•åŒ…å«è¯¥äººå‘˜çš„äººè„¸å›¾åƒ
            threshold: åŒ¹é…é˜ˆå€¼ï¼Œé»˜è®¤0.4
            quality_threshold: äººè„¸è´¨é‡é˜ˆå€¼ï¼Œé»˜è®¤0.7
            device: è®¡ç®—è®¾å¤‡ï¼Œ'auto'/'cpu'/'gpu'
            rebuild_gallery: æ˜¯å¦å¼ºåˆ¶é‡å»ºå›¾åº“
        """
        self.detection_model = detection_model
        self.recognition_model = recognition_model
        self.gallery_path = Path(gallery_path)
        self.threshold = threshold
        self.quality_threshold = quality_threshold
        self.det_size: Tuple[int, int] = (det_size, det_size,)  # æ£€æµ‹å°ºå¯¸é…ç½®
        self.device = device
        self.rebuild_gallery = rebuild_gallery
        # YOLO æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå¯åœ¨å¤–éƒ¨ä¼ å…¥ä»¥æé«˜å¬å›ï¼‰
        self.yolo_conf = float(yolo_conf)
        # YOLO æ¨ç†è¾“å…¥å°ºå¯¸ï¼ˆå•è¾¹åƒç´ ï¼‰ï¼Œå¢å¤§ä»¥æå‡å¯¹å°ç›®æ ‡çš„æ£€æµ‹ï¼ˆæ¨èï¼‰
        self.yolo_imgsz = 960
        # æ˜¯å¦å¯ç”¨å¹³é“ºæ£€æµ‹ï¼ˆå¯¹æ•™å®¤è¿™ç§å¯†é›†å°äººè„¸åœºæ™¯æœ‰å¸®åŠ©ï¼Œæ¨èå¯ç”¨ï¼‰
        self.yolo_use_tiling = True
        # å¹³é“ºå¤§å°ä¸é‡å ï¼ˆåƒç´ ï¼‰ï¼Œæ¨è tile_size=800, overlap=0.25
        self.yolo_tile_size = 800
        self.yolo_tile_overlap = 0.25
        # åˆå¹¶é‡å¤æ¡†çš„ IoU é˜ˆå€¼ï¼ˆNMSï¼‰ï¼Œæ¨è 0.25
        self.yolo_nms_thresh = 0.25

        # æ¨¡å‹å®ä¾‹
        self._detect_app: Optional[YOLO] = None
        self._recogn_app: Optional[FaceAnalysis] = None

        # å›¾åº“embeddings
        self.gallery_embeddings = {}
        self.gallery_stats = {}  # å­˜å‚¨è´¨é‡ç»Ÿè®¡ä¿¡æ¯

        # è¿è¡Œå‚æ•°
        self.ctx_id = -1  # -1è¡¨ç¤ºCPUï¼Œ0è¡¨ç¤ºç¬¬ä¸€ä¸ªGPU

        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()
        self._load_or_build_gallery()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–InsightFaceæ¨¡å‹"""
        try:
            # é€‰æ‹©è®¾å¤‡
            if self.device == "auto":
                device_cnt = torch.cuda.device_count()
                device = "gpu" if device_cnt > 0 else "cpu"
            else:
                device = self.device

            # é…ç½®providersï¼Œä½¿ç”¨å½“å‰ç©ºé—²çš„GPU
            if device == "gpu":
                providers = ["CUDAExecutionProvider"]
                self.ctx_id = 0  # é€‰æ‹©ç¬¬ä¸€ä¸ªGPU
            else:
                providers = ["CPUExecutionProvider"]
                self.ctx_id = -1

            # åœ¨åˆ›å»ºæ¨¡å‹æ—¶ä¹ŸæŠ‘åˆ¶å…¶å¯èƒ½çš„åº•å±‚è¾“å‡º
            with _suppress_fds():
                self._detect_app = YOLO(self.detection_model)
            logger.info(f"å·²åŠ è½½ YOLO æ¨¡å‹: {self.detection_model}")

            with _suppress_fds():
                self._recogn_app = FaceAnalysis(
                    name=self.recognition_model,
                    providers=providers,
                    allowed_modules=["detection", "recognition"],
                )
            logger.info(f"å·²åŠ è½½ InsightFace æ¨¡å‹: {self.recognition_model}")

            # ä½¿ç”¨ç»Ÿä¸€çš„ det_size è¿›è¡Œ prepare
            buf = io.StringIO()
            with redirect_stdout(buf), redirect_stderr(buf):
                self._recogn_app.prepare(ctx_id=self.ctx_id, det_size=self.det_size)
        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _build_gallery(self):
        """æ„å»ºå›¾åº“"""
        logger.info("å¼€å§‹æ„å»ºå›¾åº“...")

        total_images = 0
        successful_images = 0
        
        for person_dir in self.gallery_path.iterdir():
            if not person_dir.is_dir():
                continue
            
            person_name = person_dir.name
            # é‡‡ç”¨ä¸åŒºåˆ†å¤§å°å†™çš„åç¼€åŒ¹é…ï¼Œé¿å…æ¼æ‰åƒ 0001.JPG è¿™ç§å¤§å†™æ‰©å±•å
            image_files = [p for p in person_dir.iterdir() if p.is_file() and p.suffix.lower() in ('.jpg', '.jpeg', '.png')]

            logger.info(f"å¤„ç† {person_name}: {len(image_files)} å¼ å›¾åƒ")
            total_images += len(image_files)
            
            person_embeddings = []
            person_qualities = []
            
            for img_file in image_files:
                try:
                    image = cv2.imread(str(img_file))
                    if image is None:
                        logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_file}")
                        continue
                    
                    # é²æ£’äººè„¸æ£€æµ‹
                    faces = self.detect_faces(image)
                    
                    if len(faces) == 0:
                        logger.warning(f"åœ¨ {img_file} ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
                        continue
                    
                    # é€‰æ‹©è´¨é‡æœ€é«˜çš„äººè„¸
                    best_face = None
                    best_quality = 0
                    
                    for face in faces:
                        quality = self.assess_face_quality(face, image.shape)
                        if quality > best_quality:
                            best_quality = quality
                            best_face = face
                    
                    if best_face and best_quality >= self.quality_threshold:
                        embedding = best_face.embedding
                        person_embeddings.append(embedding)
                        person_qualities.append(best_quality)
                        
                        logger.info(f"  {img_file.name}: è´¨é‡{best_quality:.3f}, å°ºå¯¸{best_face.det_size}")
                        successful_images += 1
                    else:
                        logger.warning(f"  {img_file.name}: äººè„¸è´¨é‡è¿‡ä½ ({best_quality:.3f}), faces={len(faces)}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç† {img_file} å¤±è´¥: {e}")
            
            if person_embeddings:
                embeddings = np.array(person_embeddings)
                qualities = np.array(person_qualities)
                
                # è´¨é‡åŠ æƒå¹³å‡
                weights = qualities / qualities.sum()
                avg_embedding = np.average(embeddings, axis=0, weights=weights)
                
                self.gallery_embeddings[person_name] = avg_embedding
                self.gallery_stats[person_name] = {
                    'quality': np.mean(qualities),
                    'count': len(person_embeddings),
                    'avg_norm': np.linalg.norm(avg_embedding)
                }
                
                logger.info(f"  âœ… æˆåŠŸæ·»åŠ : {len(person_embeddings)} ä¸ªé«˜è´¨é‡embeddings")
            else:
                logger.warning(f"  âŒ {person_name} æ²¡æœ‰åˆæ ¼çš„äººè„¸å›¾åƒ")
        
        # ä¿å­˜å›¾åº“
        if self.gallery_embeddings:
            gallery_data = {
                'embeddings': self.gallery_embeddings,
                'stats': self.gallery_stats,
                'threshold': self.threshold,
                'quality_threshold': self.quality_threshold
            }
            
            embeddings_file = self.gallery_path / 'gallery_embeddings.pkl'
            with open(embeddings_file, 'wb') as f:
                pickle.dump(gallery_data, f)
            
            logger.info(f"å›¾åº“æ„å»ºå®Œæˆ: {len(self.gallery_embeddings)} ä¸ªäºº, {successful_images}/{total_images} å¼ å›¾åƒ")

    def _crop_and_resize(self, crop: np.ndarray, target: Tuple[int,int]) -> Tuple[np.ndarray, float, float]:
        """
        æŠŠ crop ç¼©æ”¾åˆ° target (w,h)ï¼Œè¿”å› (resized, sx, sy) å…¶ä¸­ sx = orig_w / resized_w ç”¨äºåå‘æ˜ å°„ã€‚
        
        Args:
            crop: è£å‰ªå›¾åƒ
            target: ç›®æ ‡å°ºå¯¸ (w, h)
        
        Returns:
            resized: ç¼©æ”¾åçš„å›¾åƒ
            sx: å®½åº¦ç¼©æ”¾æ¯”ä¾‹
            sy: é«˜åº¦ç¼©æ”¾æ¯”ä¾‹
        """
        tw, th = target
        try:
            resized = cv2.resize(crop, (tw, th), interpolation=cv2.INTER_LINEAR)
        except Exception:
            resized = crop.copy()
        # ç¼©æ”¾æ¯”ä¾‹ï¼ˆç”¨äºå°† face.bbox æ˜ å›åŸå›¾æ—¶ä½¿ç”¨ï¼‰
        sx = float(crop.shape[1]) / float(resized.shape[1]) if resized.shape[1] > 0 else 1.0
        sy = float(crop.shape[0]) / float(resized.shape[0]) if resized.shape[0] > 0 else 1.0
        return resized, sx, sy

    def _detect_with_yolo(self, image: np.ndarray, conf: float = None) -> List[Tuple[int,int,int,int]]:
        """
        ä½¿ç”¨ YOLO æ£€æµ‹äººè„¸ï¼Œè¿”å› xyxy åˆ—è¡¨ï¼ˆæ•´æ•°ï¼‰
        
        Args:
            image: éœ€è¦æ£€æµ‹çš„å›¾ç‰‡
            conf: ç½®ä¿¡åº¦
        
        Returns:
            boxes: è¯†åˆ«çš„å›¾ç‰‡åæ ‡
        """
        boxes = []
        if self._detect_app is None:
            return boxes

        try:
            # æ”¯æŒä»å®ä¾‹é»˜è®¤å€¼è¦†ç›–é˜ˆå€¼
            if conf is None:
                conf = float(self.yolo_conf)
            # ä½¿ç”¨æ›´å¤§çš„æ¨ç†å°ºå¯¸ä»¥æå‡å¯¹å°äººè„¸çš„æ£€æµ‹èƒ½åŠ›
            with _suppress_fds():
                try:
                    results = self._detect_app(image, imgsz=self.yolo_imgsz)
                except TypeError:
                    # å…¼å®¹ä¸åŒ ultralytics ç‰ˆæœ¬çš„å‚æ•°ç­¾å
                    results = self._detect_app(image)

            if not results:
                return boxes

            r = results[0]
            # ä»…æ”¯æŒç°ä»£è¿”å›æ ¼å¼ï¼ˆ.boxes æ¯é¡¹åŒ…å« .xyxy å’Œ .confï¼‰ï¼Œç®€åŒ–å…¼å®¹é€»è¾‘
            for box in r.boxes:
                xyxy = box.xyxy.cpu().numpy() if hasattr(box.xyxy, 'cpu') else np.array(box.xyxy)
                if xyxy.ndim == 2:
                    x1, y1, x2, y2 = map(int, xyxy[0])
                else:
                    x1, y1, x2, y2 = map(int, xyxy)
                score = float(box.conf)
                if score >= conf:
                    boxes.append((x1, y1, x2, y2))

        except Exception as e:
            logger.warning(f"YOLO æ£€æµ‹å¤±è´¥: {e}")

        return boxes

    def _detect_with_yolo_batch(self, images: List[np.ndarray], conf: float = None) -> List[List[Tuple[int, int, int, int]]]:
        """ä½¿ç”¨ YOLO å¯¹ä¸€æ‰¹å›¾åƒè¿›è¡Œæ£€æµ‹ï¼Œè¿”å›æ¯å¼ å›¾åƒçš„ bbox åˆ—è¡¨ã€‚

        Args:
            images: BGR å›¾åƒåˆ—è¡¨
            conf: ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            æ¯å¼ å›¾åƒå¯¹åº”çš„ bbox åˆ—è¡¨ï¼Œå…ƒç´ å½¢å¦‚ [(x1, y1, x2, y2), ...]
        """
        if self._detect_app is None or not images:
            return [[] for _ in images]

        boxes_batch: List[List[Tuple[int, int, int, int]]] = [[] for _ in images]
        try:
            # æ”¯æŒä»å®ä¾‹é»˜è®¤å€¼è¦†ç›–é˜ˆå€¼
            if conf is None:
                conf = float(self.yolo_conf)
            # ä½¿ç”¨ FD çº§åˆ«æŠ‘åˆ¶ï¼Œèƒ½è¦†ç›–å¤šçº¿ç¨‹ / C å±‚æ‰“å°
            with _suppress_fds():
                results = self._detect_app(images)
            # Ultralytics YOLO å¯èƒ½è¿”å›å•ä¸ªæˆ–åˆ—è¡¨ï¼Œè¿™é‡Œç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨
            if not isinstance(results, (list, tuple)):
                results = [results]

            for idx, (img, r) in enumerate(zip(images, results)):
                cur_boxes: List[Tuple[int, int, int, int]] = []
                try:
                        if r is None or not hasattr(r, 'boxes') or r.boxes is None:
                            boxes_batch[idx] = []
                            continue
                        for box in r.boxes:
                            xyxy = box.xyxy.cpu().numpy() if hasattr(box.xyxy, 'cpu') else np.array(box.xyxy)
                            if xyxy.ndim == 2:
                                x1, y1, x2, y2 = map(int, xyxy[0])
                            else:
                                x1, y1, x2, y2 = map(int, xyxy)
                            score = float(box.conf)
                            if score >= conf:
                                cur_boxes.append((x1, y1, x2, y2))
                except Exception as e:
                    logger.warning(f"YOLO æ‰¹é‡æ£€æµ‹å¤±è´¥: index={idx}, error={e}")
                boxes_batch[idx] = cur_boxes

        except Exception as e:
            logger.warning(f"YOLO æ‰¹é‡æ£€æµ‹è°ƒç”¨å¤±è´¥: {e}")
            return [[] for _ in images]

        return boxes_batch

    def _boxes_nms(self, boxes_with_scores: List[Tuple[int, int, int, int, float]], iou_thresh: float) -> List[Tuple[int, int, int, int]]:
        """ç®€å• NMSï¼šæŒ‰ score é™åºï¼Œä¿ç•™ä¸å½“å‰ä¿ç•™æ¡† IoU < thresh çš„æ¡†ã€‚"""
        if not boxes_with_scores:
            return []
        boxes_with_scores = sorted(boxes_with_scores, key=lambda x: x[4], reverse=True)
        keep: List[Tuple[int, int, int, int]] = []
        for b in boxes_with_scores:
            x1, y1, x2, y2, s = b
            discard = False
            for k in keep:
                # è®¡ç®— IoU
                xx1 = max(x1, k[0])
                yy1 = max(y1, k[1])
                xx2 = min(x2, k[2])
                yy2 = min(y2, k[3])
                w = max(0, xx2 - xx1)
                h = max(0, yy2 - yy1)
                inter = w * h
                area_a = max(0, x2 - x1) * max(0, y2 - y1)
                area_b = max(0, k[2] - k[0]) * max(0, k[3] - k[1])
                denom = area_a + area_b - inter
                iou = inter / denom if denom > 0 else 0.0
                if iou >= iou_thresh:
                    discard = True
                    break
            if not discard:
                keep.append((x1, y1, x2, y2))
        return keep

    def _detect_with_yolo_tiled(self, image: np.ndarray, conf: float = None) -> List[Tuple[int, int, int, int]]:
        """å¯¹å¤§å›¾å¯ç”¨å¹³é“ºæ£€æµ‹ï¼Œæå‡å¯¹å°äººè„¸çš„å¬å›ã€‚

        è¿”å› xyxy æ¡†åˆ—è¡¨ï¼Œä¸ `_detect_with_yolo` ä¿æŒä¸€è‡´ã€‚
        """
        if self._detect_app is None:
            return []
        if conf is None:
            conf = float(self.yolo_conf)

        h, w = image.shape[:2]
        tile = int(self.yolo_tile_size)
        overlap = float(self.yolo_tile_overlap)
        step = max(1, int(tile * (1.0 - overlap)))

        collected: List[Tuple[int, int, int, int, float]] = []

        xs = list(range(0, w, step))
        ys = list(range(0, h, step))
        for yy in ys:
            for xx in xs:
                x1 = xx
                y1 = yy
                x2 = min(w, xx + tile)
                y2 = min(h, yy + tile)
                crop = image[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                try:
                    # ä¸å•å›¾ç›¸åŒçš„ imgsz å‚æ•°å°è¯•ï¼ˆä½¿ç”¨ FD çº§åˆ«æŠ‘åˆ¶ï¼Œä»¥å±è”½ ultralytics è¾“å‡ºï¼‰
                    with _suppress_fds():
                        try:
                            results = self._detect_app(crop, imgsz=self.yolo_imgsz)
                        except TypeError:
                            results = self._detect_app(crop)
                    if not results:
                        continue
                    r = results[0]
                    if r is None or not hasattr(r, 'boxes') or r.boxes is None:
                        continue
                    for box in r.boxes:
                        xyxy = box.xyxy.cpu().numpy() if hasattr(box.xyxy, 'cpu') else np.array(box.xyxy)
                        if xyxy.ndim == 2:
                            bx1, by1, bx2, by2 = map(int, xyxy[0])
                        else:
                            bx1, by1, bx2, by2 = map(int, xyxy)
                        score = float(box.conf)
                        if score >= conf:
                            # è½¬å›åŸå›¾åæ ‡
                            gx1 = x1 + bx1
                            gy1 = y1 + by1
                            gx2 = x1 + bx2
                            gy2 = y1 + by2
                            collected.append((gx1, gy1, gx2, gy2, score))
                except Exception:
                    continue

        # NMS åˆå¹¶é‡å¤æ¡†
        boxes = self._boxes_nms(collected, iou_thresh=self.yolo_nms_thresh)

        # ç®€æ´æ±‡æ€»æ—¥å¿—ï¼šå¹³é“ºæ•°ã€æ”¶é›†åˆ°çš„å€™é€‰æ¡†å’Œ NMS åæ¡†æ•°ï¼ˆä»…åœ¨è°ƒè¯•çº§åˆ«è¾“å‡ºï¼‰
        try:
            tiles_processed = len(xs) * len(ys)
            logger.debug(f"YOLO tiled: tiles={tiles_processed}, collected={len(collected)}, nms_kept={len(boxes)}")
        except Exception:
            pass

        return boxes

    def _detect_faces_with_boxes(self, image: np.ndarray, yolo_boxes: List[Tuple[int, int, int, int]]) -> List:
        """åœ¨å·²ç»™å®š YOLO bbox çš„å‰æä¸‹ï¼Œæ‰§è¡Œè£å‰ª + InsightFace è¯†åˆ«å¹¶è¿”å›äººè„¸åˆ—è¡¨ã€‚"""
        all_faces = []
        img_h, img_w = image.shape[:2]

        if not yolo_boxes:
            # è‹¥ YOLO æœªç»™å‡ºå€™é€‰æ¡†ï¼Œåˆ™å›é€€åˆ° InsightFace å¯¹æ•´å›¾æ£€æµ‹ï¼ˆå¯æ£€æµ‹å°/å¯†é›†äººè„¸ï¼‰
            try:
                faces = self._recogn_app.get(image)
                if not faces:
                    return []
                for face in faces:
                    try:
                        # face.bbox å·²ä¸ºåŸå›¾åæ ‡
                        face.det_size = (img_w, img_h)
                        face.enhancement = 'full_image'
                        all_faces.append(face)
                    except Exception:
                        continue
                logger.info(f"InsightFace æ•´å›¾æ£€æµ‹åˆ° {len(all_faces)} å¼ äººè„¸ï¼ˆå›é€€è·¯å¾„ï¼‰")
                merged = self._merge_close_faces(all_faces, image.shape, center_tol=0.012)
                return merged
            except Exception as e:
                logger.debug(f"InsightFace æ•´å›¾å›é€€å¤±è´¥: {e}")
                return []

        import time
        st = time.time()

        for (bx1, by1, bx2, by2) in yolo_boxes:
            bw = bx2 - bx1
            bh = by2 - by1
            pad_x = int(bw * 0.15)
            pad_y = int(bh * 0.15)

            x1 = max(0, bx1 - pad_x)
            y1 = max(0, by1 - pad_y)
            x2 = min(img_w, bx2 + pad_x)
            y2 = min(img_h, by2 + pad_y)

            crop = image[y1:y2, x1:x2]
            if crop.size == 0:
                continue

            resized, sx, sy = self._crop_and_resize(crop, self.det_size)
            target_det = self.det_size

            try:
                faces = self._recogn_app.get(resized)
            except Exception as e:
                logger.debug(f"InsightFace å¯¹è£å‰ªå›¾åƒå¤„ç†å¤±è´¥: {e}")
                continue

            for face in faces:
                try:
                    bbox = face.bbox.astype(float)
                    ox1 = int(bbox[0] * sx) + x1
                    oy1 = int(bbox[1] * sy) + y1
                    ox2 = int(bbox[2] * sx) + x1
                    oy2 = int(bbox[3] * sy) + y1

                    face.bbox = np.array([ox1, oy1, ox2, oy2])

                    if hasattr(face, 'kps') and face.kps is not None:
                        kps = np.asarray(face.kps).astype(float).reshape(-1, 2)
                        kps[:, 0] = kps[:, 0] * sx + x1
                        kps[:, 1] = kps[:, 1] * sy + y1
                        face.kps = kps

                    face.det_size = target_det
                    face.enhancement = 'unified_crop'

                    all_faces.append(face)
                except Exception:
                    continue

        ed = time.time()
        logger.info(f"äººè„¸è¯†åˆ«è€—æ—¶: {ed - st:.2f} ç§’ï¼Œæ£€æµ‹åˆ° {len(all_faces)} å¼ äººè„¸")

        try:
            merged = self._merge_close_faces(all_faces, image.shape, center_tol=0.03)
            return merged
        except Exception:
            return all_faces

    def _draw(self, image: np.ndarray, result: Dict, person: int = None):
        """
        ç»“æœç»˜åˆ¶
        
        Args:
            image: éœ€è¦ç»˜åˆ¶çš„å›¾ç‰‡
            result: è¯†åˆ«ç»“æœå­—å…¸
            person: å¯é€‰ï¼Œäººå‘˜ç´¢å¼•
        """
        x1, y1, x2, y2 = result['bbox']
        identity = result['identity']
        similarity = result['similarity']
        quality = result['quality']
        
        # æ ¹æ®èº«ä»½å’Œè´¨é‡é€‰æ‹©é¢œè‰²
        if identity == "æœªçŸ¥":
            color = (0, 0, 255)  # çº¢è‰²
        elif quality >= 0.8:
            color = (0, 255, 0)  # ç»¿è‰²ï¼ˆé«˜è´¨é‡ï¼‰
        elif quality >= 0.6:
            color = (0, 255, 255)  # é»„è‰²ï¼ˆä¸­ç­‰è´¨é‡ï¼‰
        else:
            color = (0, 165, 255)  # æ©™è‰²ï¼ˆä½è´¨é‡ï¼‰
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶å…³é”®ç‚¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if result['landmarks'] is not None:
            landmarks = result['landmarks'].astype(int)
            for (x, y) in landmarks:
                cv2.circle(image, (x, y), 2, (0, 255, 255), -1)
        
        # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬ï¼ˆå¯èƒ½å«ä¸­æ–‡ï¼‰
        label = f"{identity} ({similarity:.3f})"
        quality_label = f"Q:{quality:.2f}"

        # åŸºäºäººè„¸ bbox é«˜åº¦è®¡ç®—å­—ä½“å¤§å°ï¼ˆç›¸å¯¹æ¯”ä¾‹ï¼‰ï¼Œå¹¶é™åˆ¶åœ¨åˆç†èŒƒå›´
        face_h = max(12, y2 - y1)
        # æ ‡ç­¾å­—ä½“å–äººè„¸é«˜åº¦çš„ 18%ï¼Œè´¨é‡å­—ä½“å– 14%
        label_font_size = max(12, int(face_h * 0.12))
        quality_font_size = max(10, int(face_h * 0.08))

        # æµ‹é‡æ–‡æœ¬å°ºå¯¸ä»¥ç»˜åˆ¶èƒŒæ™¯
        text_w, text_h = measure_text_cn(label, label_font_size)
        padding_x = max(6, int(label_font_size * 0.3))
        padding_y = max(4, int(label_font_size * 0.2))

        bg_x1 = x1
        bg_y1 = max(0, y1 - text_h - padding_y * 2)
        bg_x2 = x1 + text_w + padding_x * 2
        bg_y2 = y1

        cv2.rectangle(image, (bg_x1, bg_y1), (bg_x2, bg_y2), color, -1)

        # åœ¨èƒŒæ™¯å†…ç»˜åˆ¶æ ‡ç­¾ï¼ˆä½¿ç”¨å†…è¾¹è·ï¼‰
        text_org = (bg_x1 + padding_x, bg_y1 + padding_y)
        draw_text_cn(image, label, text_org, font_size=label_font_size, color=(255, 255, 255))

        # ç»˜åˆ¶è´¨é‡ä¿¡æ¯ï¼ˆæ”¾åœ¨ bbox åº•éƒ¨ï¼‰
        q_w, q_h = measure_text_cn(quality_label, quality_font_size)
        q_org = (x1, min(image.shape[0] - q_h - 2, y1 + 15))
        draw_text_cn(image, quality_label, q_org, font_size=quality_font_size, color=(255, 255, 255))

    def _load_or_build_gallery(self):
        """åŠ è½½æˆ–æ„å»ºå›¾åº“"""
        embeddings_file = self.gallery_path / 'gallery_embeddings.pkl'
        
        if embeddings_file.exists() and not self.rebuild_gallery:
            try:
                with open(embeddings_file, 'rb') as f:
                    gallery_data = pickle.load(f)
                
                self.gallery_embeddings = gallery_data['embeddings']
                self.gallery_stats = gallery_data.get('stats', {})
                
                logger.info(f"å·²åŠ è½½å›¾åº“: {len(self.gallery_embeddings)} ä¸ªäºº")
                # self.analyze_gallery_quality()
                
            except Exception as e:
                logger.warning(f"åŠ è½½å›¾åº“å¤±è´¥: {e}")
                self._build_gallery()
        else:
            self._build_gallery()

    def _merge_close_faces(self, faces: List, image_shape: Tuple[int,int], center_tol: float = 0.012) -> List:
        """
        åˆå¹¶åœ¨å›¾åƒä¸­ä½ç½®éå¸¸æ¥è¿‘çš„å€™é€‰äººè„¸ï¼Œé¿å…é‡å¤ç»˜åˆ¶ã€‚

        Args:
            faces: InsightFace è¿”å›çš„ face å¯¹è±¡åˆ—è¡¨ï¼ˆå« bboxï¼‰
            image_shape: åŸå›¾ shape (h, w, ...)
            center_tol: ä¸­å¿ƒç‚¹è·ç¦»é˜ˆå€¼ï¼ŒæŒ‰æœ€å¤§è¾¹é•¿çš„æ¯”ä¾‹è®¡ç®—ï¼Œè‹¥ä¸¤æ¡†ä¸­å¿ƒè·ç¦»å°äº tol*max_dim åˆ™åˆå¹¶

        Returns:
            faces: åˆå¹¶åçš„ face åˆ—è¡¨ï¼Œä¿ç•™æ¯ç»„ä¸­è´¨é‡æœ€é«˜çš„ faceï¼ˆä½¿ç”¨ assess_face_quality è¯„åˆ†ï¼‰ã€‚
        """
        if not faces or len(faces) <= 1:
            return faces

        h, w = image_shape[:2]
        max_dim = max(w, h)
        tol = center_tol * max_dim

        centers = []
        for f in faces:
            try:
                bx = f.bbox.astype(float)
                cx = (bx[0] + bx[2]) / 2.0
                cy = (bx[1] + bx[3]) / 2.0
            except Exception:
                # å…œåº•
                cx, cy = 0.0, 0.0
            centers.append((cx, cy))

        used = [False] * len(faces)
        merged = []

        for i, fi in enumerate(faces):
            if used[i]:
                continue
            group = [i]
            used[i] = True
            for j in range(i + 1, len(faces)):
                if used[j]:
                    continue
                dist = np.linalg.norm(np.array(centers[i]) - np.array(centers[j]))
                if dist <= tol:
                    group.append(j)
                    used[j] = True

            # ä» group ä¸­é€‰å“è´¨æœ€é«˜çš„ face ä¿ç•™
            best_idx = group[0]
            try:
                best_score = self.assess_face_quality(faces[best_idx], image_shape)
            except Exception:
                best_score = getattr(faces[best_idx], 'det_score', 0.5)

            for idx in group[1:]:
                try:
                    score = self.assess_face_quality(faces[idx], image_shape)
                except Exception:
                    score = getattr(faces[idx], 'det_score', 0.5)
                if score > best_score:
                    best_score = score
                    best_idx = idx

            merged.append(faces[best_idx])

        return merged

    def analyze_gallery_quality(self):
        """
        åˆ†æå›¾åº“è´¨é‡ã€‚
        è¾“å‡ºå›¾åº“ä¸­æ¯ä¸ªäººçš„è´¨é‡ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¹³å‡è´¨é‡ã€æ ·æœ¬æ•°é‡å’ŒEmbeddingèŒƒæ•°ã€‚
        """
        if not self.gallery_stats:
            return

        logger.info("=== å›¾åº“è´¨é‡åˆ†æ ===")

        for person_name, stats in self.gallery_stats.items():
            logger.info(f"{person_name}:")
            logger.info(f"  å¹³å‡è´¨é‡: {stats['quality']:.3f}")
            logger.info(f"  æ ·æœ¬æ•°é‡: {stats['count']}")
            logger.info(f"  EmbeddingèŒƒæ•°: {stats['avg_norm']:.3f}")

        # è®¡ç®—ç±»é—´ç›¸ä¼¼åº¦
        if len(self.gallery_embeddings) >= 2:
            logger.info("ç±»é—´ç›¸ä¼¼åº¦åˆ†æ:")
            names = list(self.gallery_embeddings.keys())
            
            for i in range(len(names)):
                for j in range(i + 1, len(names)):
                    emb1 = self.gallery_embeddings[names[i]]
                    emb2 = self.gallery_embeddings[names[j]]
                    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                    logger.info(f"  {names[i]} vs {names[j]}: {similarity:.4f}")

    def assess_face_quality(self, face, image_shape: Tuple[int, int]) -> float:
        """
        è¯„ä¼°äººè„¸è´¨é‡

        Args:
            face: æ£€æµ‹åˆ°çš„äººè„¸
            image_shape: å›¾åƒå°ºå¯¸ (height, width)

        Returns:
            quality_score: äººè„¸è´¨é‡åˆ†æ•° (0-1)
        """
        quality_score = 0.5  # åŸºç¡€åˆ†æ•°

        try:
            # 1. äººè„¸å°ºå¯¸è¯„ä¼°
            bbox = face.bbox
            face_width = bbox[2] - bbox[0]
            face_height = bbox[3] - bbox[1]
            
            # è®¡ç®—ç›¸å¯¹å°ºå¯¸
            img_height, img_width = image_shape[:2]
            relative_size = (face_width * face_height) / (img_width * img_height)
            
            # è¯¾å ‚åœºæ™¯ï¼šè¿œè·å°è„¸æ˜¯å¸¸æ€ï¼Œç®€åŒ–å°ºå¯¸è¯„ä¼°
            if relative_size >= 0.02:
                quality_score += 0.15
            else:
                quality_score -= 0.05  # ä»…è½»å¾®æƒ©ç½šæå°äººè„¸
            
            # 2. æ£€æµ‹ç½®ä¿¡åº¦
            det_score = getattr(face, 'det_score', 0.8)
            quality_score += (det_score - 0.5) * 0.3
            
            # 3. å§¿æ€è¯„ä¼°ï¼ˆå¦‚æœæœ‰å…³é”®ç‚¹ä¿¡æ¯ï¼‰
            if hasattr(face, 'kps') and face.kps is not None:
                kps = face.kps
                # ç®€å•çš„å§¿æ€è¯„ä¼°ï¼šæ£€æŸ¥çœ¼ç›ä½ç½®
                left_eye, right_eye = kps[0], kps[1]
                eye_distance = np.linalg.norm(right_eye - left_eye)
                
                # ç†æƒ³çš„çœ¼ç›è·ç¦»
                if eye_distance > face_width * 0.3:
                    quality_score += 0.1
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
            quality_score = max(0.1, min(1.0, quality_score))
            
        except Exception as e:
            logger.warning(f"è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            quality_score = 0.5
        
        return quality_score

    def detect_faces(self, image: np.ndarray) -> List:
        """
        ä»å›¾åƒä¸­æ£€æµ‹äººè„¸

        Args:
            image: è¾“å…¥å›¾åƒ

        Returns:
            all_faces: æ£€æµ‹åˆ°çš„æ‰€æœ‰äººè„¸åˆ—è¡¨
        """
        # æŒ‰ç…§ YOLO -> è£å‰ª -> ç»Ÿä¸€ç¼©æ”¾ -> InsightFace çš„æµç¨‹æ‰§è¡Œäººè„¸è¯†åˆ«
        if self.yolo_use_tiling:
            yolo_boxes = self._detect_with_yolo_tiled(image)
        else:
            yolo_boxes = self._detect_with_yolo(image)
        logger.debug(f"YOLO æ£€æµ‹åˆ° {len(yolo_boxes)} ä¸ªå€™é€‰æ¡†ï¼Œä½¿ç”¨ç»Ÿä¸€æµç¨‹è¿›è¡Œè¯†åˆ«")
        return self._detect_faces_with_boxes(image, yolo_boxes)

    def detect_faces_batch(self, images: List[np.ndarray]) -> List[List]:
        """å¯¹å¤šå¼ å›¾åƒæ‰§è¡Œäººè„¸æ£€æµ‹ä¸è¯†åˆ«ï¼Œå°½å¯èƒ½åœ¨ YOLO ä¾§åš batchï¼Œä»¥æé«˜ GPU åˆ©ç”¨ç‡ã€‚

        Args:
            images: BGR å›¾åƒåˆ—è¡¨

        Returns:
            æ¯å¼ å›¾åƒå¯¹åº”çš„äººè„¸åˆ—è¡¨ï¼ˆä¸ detect_faces å•å›¾æ¥å£å…¼å®¹ï¼‰
        """
        if not images:
            return []

        faces_batch: List[List] = []
        if self.yolo_use_tiling:
            for img in images:
                yolo_boxes = self._detect_with_yolo_tiled(img)
                faces = self._detect_faces_with_boxes(img, yolo_boxes)
                faces_batch.append(faces)
            return faces_batch

        yolo_boxes_batch = self._detect_with_yolo_batch(images)
        for img, yolo_boxes in zip(images, yolo_boxes_batch):
            faces = self._detect_faces_with_boxes(img, yolo_boxes)
            faces_batch.append(faces)
        return faces_batch

    def get_gallery_info(self) -> Dict:
        """è·å–å›¾åº“ä¿¡æ¯"""
        info = {
            'total_persons': len(self.gallery_embeddings),
            'person_names': list(self.gallery_embeddings.keys()),
            'similarity_threshold': self.threshold,
            'quality_threshold': self.quality_threshold,
            'stats': self.gallery_stats
        }
        
        return info

    def proccess(self, image_path: str, output_path: Optional[str] = None) -> Tuple[np.ndarray, List[Dict]]:
        """
        äººè„¸è¯†åˆ«
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            output_path: è¾“å‡ºå›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            result_image: å¸¦æ ‡æ³¨çš„ç»“æœå›¾åƒ
            recognition_results: è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        
        result_image = image.copy()
        
        # äººè„¸æ£€æµ‹
        faces = self.detect_faces(image)

        if len(faces) == 0:
            logger.warning(f"åœ¨ {image_path} ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
            if output_path:
                cv2.imwrite(output_path, result_image)
            return result_image, []
        
        recognition_results = []
        
        for i, face in enumerate(faces):
            # è¯„ä¼°è´¨é‡
            quality = self.assess_face_quality(face, image.shape)
            
            # è·å–äººè„¸æ¡†
            bbox = face.bbox.astype(int)
            x1, y1, x2, y2 = bbox

            # è¯†åˆ«èº«ä»½
            identity, similarity = self.recognize_identity(face.embedding, quality)

            # å‡†å¤‡ç»“æœ
            result = {
                'bbox': (x1, y1, x2, y2),
                'identity': identity,
                'similarity': similarity,
                'quality': quality,
                'landmarks': getattr(face, 'kps', None),
                'det_size': getattr(face, 'det_size', None),
                'enhancement': getattr(face, 'enhancement', 'original')
            }
            
            recognition_results.append(result)
            
            # ç»˜åˆ¶ç»“æœ
            self._draw(result_image, result, i)
            
            logger.info(f"æ£€æµ‹åˆ°äººè„¸ {i+1}: {identity} (ç›¸ä¼¼åº¦: {similarity:.4f}, è´¨é‡: {quality:.3f})")
        
        # ä¿å­˜ç»“æœ
        if output_path:
            cv2.imwrite(output_path, result_image)
            logger.info(f"ç»“æœå›¾åƒå·²ä¿å­˜è‡³: {output_path}")
        
        return result_image, recognition_results

    def recognize_identity(self, embedding: np.ndarray, quality: float, debug: bool = False, topk: int = 5) -> Tuple[str, float]:
        """
        èº«ä»½è¯†åˆ«ï¼Œè€ƒè™‘è´¨é‡å› ç´ 
        
        Args:
            embedding: äººè„¸embedding
            quality: äººè„¸è´¨é‡åˆ†æ•°
            
        Returns:
            (èº«ä»½, ç›¸ä¼¼åº¦)
        """
        if not self.gallery_embeddings:
            return "æœªçŸ¥", 0.0
        
        best_match = "æœªçŸ¥"
        best_similarity = 0.0

        # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦å¹¶æ’åºï¼ˆç”¨äºè°ƒè¯• / é˜ˆå€¼è°ƒæ•´å‚è€ƒï¼‰
        sims = []
        try:
            emb_norm = np.linalg.norm(embedding)
        except Exception:
            emb_norm = 0.0

        for person_name, gallery_embedding in self.gallery_embeddings.items():
            try:
                g_norm = np.linalg.norm(gallery_embedding)
                if emb_norm == 0 or g_norm == 0:
                    similarity = 0.0
                else:
                    similarity = float(np.dot(embedding, gallery_embedding) / (emb_norm * g_norm))
            except Exception:
                similarity = 0.0
            sims.append((person_name, similarity))

        sims.sort(key=lambda x: x[1], reverse=True)

        if sims:
            best_match, best_similarity = sims[0]

        # è°ƒè¯•è¾“å‡º top-k ç›¸ä¼¼åº¦ï¼Œä¾¿äºè§‚å¯Ÿç›¸ä¼¼åº¦åˆ†å¸ƒ
        if debug:
            topk = max(1, int(topk))
            logger.info(f"recognize_identity debug: top{topk} -> {sims[:topk]}")

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if best_similarity >= self.threshold:
            return best_match, best_similarity
        else:
            return "æœªçŸ¥", best_similarity




# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå¢å¼ºç‰ˆè¯†åˆ«å™¨çš„ä½¿ç”¨"""
    
    # åˆ›å»ºå¢å¼ºç‰ˆè¯†åˆ«å™¨
    recognizer = FaceRecognizer(
        gallery_path='data/id_photos',
        threshold=0.4,  # é™ä½é˜ˆå€¼ä»¥æé«˜å¬å›ç‡
        quality_threshold=0.6,  # é€‚ä¸­çš„è´¨é‡è¦æ±‚
        det_size=320,
        device='auto',
        rebuild_gallery=True,  # å¼ºåˆ¶é‡å»ºå›¾åº“ä»¥ç¡®ä¿æœ€æ–°æ•°æ®
    )

    # æµ‹è¯•å›¾åƒ
    test_images = [
        'hangzhou.jpeg'  # æµ‹è¯•å›¾åƒ
    ]
    
    logger.info("=== äººè„¸è¯†åˆ«æµ‹è¯• ===")
    
    for img_path in test_images:
        if not Path(img_path).exists():
            continue
        
        logger.info(f"ğŸ” æµ‹è¯•: {img_path}")
        
        try:
            # è¯†åˆ«
            result_img, results = recognizer.proccess(
                img_path,
                f'output_{Path(img_path).stem}.jpg'
            )

            if len(results) == 0:
                logger.warning("æœªæ£€æµ‹åˆ°äººè„¸")
            else:
                logger.info(f"æ£€æµ‹åˆ° {len(results)} ä¸ªäººè„¸")

                for i, result in enumerate(results):
                    identity = result['identity']
                    similarity = result['similarity']
                    quality = result['quality']

                    if identity == "æœªçŸ¥":
                        logger.info(f"äººè„¸ {i+1}: {identity} (ç›¸ä¼¼åº¦: {similarity:.4f}, è´¨é‡: {quality:.3f})")
                    else:
                        logger.info(f"äººè„¸ {i+1}: {identity} (ç›¸ä¼¼åº¦: {similarity:.4f}, è´¨é‡: {quality:.3f}) âœ…")

        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()
